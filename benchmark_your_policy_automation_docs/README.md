# How to Benchmark your Policy automation against reference dataset
P.S. The naming conventions used in this file are not mandatory and are introduced solely to facilitate understanding.

To generate new policy cases and test them on an existing dataset, follow these guidelines:

1. **Create/Generate the `{n}_{policy_name}_policy.py`** file, where `n` is the policy example number written in words (e.g., `one_luggage_compliance.py`):  
   - This file determines whether a case is eligible according to the policy text document.  
   - The code can be:
     1. Fully manually written fully.
     2. Generated using an LLM (Granite, Llama, Mistral, GPT, or any other) by adapting the [policy prompt template](prompt_template_benchmark_policyautomation):  
        - Update the values in `{}` (curly brackets) within the template.  
        - Copy and paste the policy text document.  
        - If other classes from preexisting policy cases are needed, copy and paste them instead of `{predefined_data_structures}`. Otherwise, remove this point.
     3. Generated by an LLM and modified manually.
     4. A direct call to an LLM.

2. **Create `{n}_{policy_name}_policytester.py`**:  
   - Test the alignment of the generated data with the `Policy` class from `{policy_name}_policy.py`.  
   - Adapt the code from the [tester template](../policy_corpus_extension_docs/tester_template.md) or use a pre-existing policy tester inside the policy folder, replacing the `Compliance` class with the new one.  
   - Add custom evaluators if the statistical metrics do not yield the desired results.

3. **Run `{n}_{policy_name}_policytester.py`**:  
   - If the returned metrics are `< 1.0`, investigate the errors and fix the `{n}_{policy_name}_policy.py`.  
   - If all metrics equal `1.0`, congratulations! You have successfully benchmarked a new policy.
